{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc4d53f-2b13-43a1-a554-82b06b0cf764",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaedfdf-fc0d-4ff0-b21d-a9097fb241fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"consumer\")\n",
    "\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965dc3-a7c7-40ea-bfdb-1e02340b3713",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark with Kafak Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed753070-30f8-4883-97aa-4147a6532147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Configuration and set application name\n",
    "conf = SparkConf().setAppName(\"KafkaExp\")\n",
    "\n",
    "# Default pyspark installation lacks kafka consumer libraries. Install kafka-client libs manually\n",
    "kafka_packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{\"2.12\"}:{\"3.3.0\"}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0',\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.0\",\n",
    "    \"com.google.guava:guava:21.0\",\n",
    "    \"org.apache.httpcomponents:httpcore:4.4.8\"\n",
    "]\n",
    "\n",
    "# Provide kafka jar paths to driver and executors\n",
    "kafka_jar_paths = '/mnt/home/prathyush/.ivy2/jars/'.join([\n",
    "    \"org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
    "    \"org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
    "    \"hadoop-aws-2.7.5.jar\",\n",
    "    \"aws-java-sdk-core-1.12.268.jar\"\n",
    "])\n",
    "\n",
    "# Connect to Spark cluster (Cluster mode instead of local mode)\n",
    "conf = (conf.setMaster('spark://spark:7077')\n",
    "        .set('spark.jars.packages', ','.join(kafka_packages))\n",
    "        .set('spark.driver.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        .set('spark.executor.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        )\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "logger.info(f\"Spark Driver memory: {sc._conf.get('spark.driver.memory')}\")\n",
    "logger.info(f\"Spark Executor memory: {sc._conf.get('spark.executor.memory')}\")\n",
    "logger.info(\n",
    "    f'Loaded jars:\\n{json.dumps((sc._jsc.sc().listJars().toList().toString().replace(\"List(\", \"\").replace(\")\", \"\").split(\", \")), indent=2)}')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b4715-66b6-4a47-8511-fd57bf5a0b52",
   "metadata": {},
   "source": [
    "# 3. Test Kafka topic and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64ee87-38ef-48fc-9c3a-cb02eea054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "def test_kafka_connection(broker_conf:dict) -> None:\n",
    "    \"\"\"\n",
    "    Function to test kafka connection\n",
    "    :param broker_conf: Broker configuration\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    client = AdminClient(broker_conf)\n",
    "    topics = client.list_topics().topics\n",
    "    if not topics:\n",
    "        raise RuntimeError()\n",
    "    print(\"Kafka Connection successful!\")\n",
    "\n",
    "\n",
    "broker_conf = {\n",
    "    'bootstrap.servers': config[\"global\"][\"kafka_bootstrap_servers\"]\n",
    "}\n",
    "\n",
    "# Test kafka connection\n",
    "test_kafka_connection(broker_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b03-56c0-45d2-8e3f-8e96fb354d86",
   "metadata": {},
   "source": [
    "# 4. Load Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ef60c-be21-4f7c-be0d-d02115d52d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema \n",
    "schema = pickle.load(open(\"schema.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3505e-9cef-4148-a0af-10643d9d94c7",
   "metadata": {},
   "source": [
    "# 5. Configure Spark-Kafka consumer options and Subscribe to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c1b40-46a6-4629-834e-5df667667891",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure spark kafka client options\n",
    "spark_kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": config[\"global\"][\"kafka_bootstrap_servers\"],\n",
    "    \"subscribe\": config[\"global\"][\"kafka_topic\"],\n",
    "    \"kafka.group.id\": config[\"global\"][\"kafka_consumer_group\"],\n",
    "    \"maxOffsetsPerTrigger\": config[\"global\"][\"max_records_per_batch\"],\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "# Enable spark read stream\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b022a19-12ed-46ab-800f-97503e2a6917",
   "metadata": {},
   "source": [
    "# 6. Start spark structred streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934c6a5-164b-42e6-9e51-963b420490e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Function for processing each batch of record\n",
    "def process_batch(batch_df, batch_idx):\n",
    "    print(f\"{batch_idx} | {batch_df.count()}\")\n",
    "\n",
    "    # Process data and calculate\n",
    "    # a. age\n",
    "    # b. num_contribs\n",
    "    # c. min_max_years\n",
    "\n",
    "    ...\n",
    "\n",
    "    # Select required columns -  \"name\", \"age\", \"num_contribs\", \"min_max\"\n",
    "    ...\n",
    "\n",
    "    # Save to parquet file - result.parquet \n",
    "    ...\n",
    "    return batch_df\n",
    "\n",
    "# Structred streaming query\n",
    "query = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08164cc-18ff-4e14-82b2-6b69af325de0",
   "metadata": {},
   "source": [
    "# 7. Monitor structred streaming job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f9d7b-bd97-42db-b5c9-74373f999bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add startup delay\n",
    "time.sleep(5)\n",
    "# Update Job Status\n",
    "\n",
    "print(query.status)\n",
    "while query.status['isDataAvailable'] or query.status['isTriggerActive']:\n",
    "    print(query.status)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Stop query\n",
    "query.stop()\n",
    "\n",
    "logger.info(\"Structred streaming job completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eab687acb3ddfe264791fe74937bc8765d50ea3df4d9a9a62730aa97325aae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
